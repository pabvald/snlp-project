{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import basic libraries and dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys\n",
    "import importlib\n",
    "import sentencepiece as spm\n",
    "\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import English, Bengali\n",
    "\n",
    "sys.path.append('./scripts')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set a language"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "LANG = English # English, Bengali"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# import libraries and dependencies\n",
    "import preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "from preprocessing import raw_preprocess, split_train_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Preprocess data for sentencepiece"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# preprocess raw data to get a list of sentences.\n",
    "sentences = raw_preprocess(LANG)\n",
    "\n",
    "# write the sentences into a file, each sentence on one line.\n",
    "with open(f'data/{LANG.name}_preprocessed.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(f'{sentence}\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Train/test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# split data to train and test.\n",
    "train, test = split_train_test(sentences, LANG)\n",
    "\n",
    "# write train and test data into corresponding files.\n",
    "train_path = f'data/{LANG.name}_train.txt'\n",
    "test_path = f'data/{LANG.name}_test.txt'\n",
    "\n",
    "with open(train_path, 'w') as f:\n",
    "    f.write('\\n'.join(train))\n",
    "with open(test_path, 'w') as f:\n",
    "    f.write('\\n'.join(test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Subword segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#TODO: experiment to get the best vocab_size for subword segmentation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# import libraries and dependencies\n",
    "from collections import namedtuple\n",
    "import segmentation\n",
    "importlib.reload(segmentation)\n",
    "from segmentation import train_segmentation, encode_text_file, decode_text_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# make a profile for each granularity level.\n",
    "Segmentation_config = namedtuple('Segmentation_config', 'id vocab_size, model_type')\n",
    "seg_profiles = [\n",
    "    Segmentation_config('s1', None, 'char'), # segmentation by characters.\n",
    "    Segmentation_config('s2', 800, 'bpe'), # segmentation by subwords with small vocabulary.\n",
    "    Segmentation_config('s3', 2000, 'bpe'), # segmentation by subwords with large vocabulary.\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "for seg in seg_profiles:\n",
    "    # train segmentation on the train data\n",
    "    spm1 = train_segmentation(\n",
    "        text_file=train_path, \n",
    "        LANG=LANG, \n",
    "        vocab_size=seg.vocab_size, \n",
    "        model_type=seg.model_type)\n",
    "  \n",
    "    encoded_file = f'data/{LANG.name}_{seg.id}.txt'\n",
    "    encode_text_file(text_file=train_path, model_path=spm1, output_file=encoded_file)\n",
    "    \n",
    "    decoded_file = f'data/{LANG.name}_{seg.id}_decoded.txt'\n",
    "    decode_text_file(text_file=encoded_file, model_path=spm1, output_file=decoded_file)\n",
    "    \n",
    "    # test it on the test data\n",
    "    encoded_file = f'data/{LANG.name}_{seg.id}_test.txt'\n",
    "    encode_text_file(text_file=test_path, model_path=spm1, output_file=encoded_file)\n",
    "    \n",
    "    decoded_file = f'data/{LANG.name}_{seg.id}_decoded_test.txt'\n",
    "    decode_text_file(text_file=encoded_file, model_path=spm1, output_file=decoded_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#TODO: comment briefly on what you observe in terms of word segmentation."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Language Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import  neptune.new as neptune \n",
    "from config import NeptuneConfig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Baseline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Hyper parameters\n",
    "seg = seg_profiles[0]\n",
    "hidden = 40\n",
    "rand_seed = 1\n",
    "debug = 2\n",
    "bptt = 3\n",
    "_class = 9999\n",
    "\n",
    "# Parameters\n",
    "model_path = f'models/rnnlm/{LANG.name}_{seg.id}'\n",
    "train_path = f'../../../data/{LANG.name}_{seg.id}.txt'\n",
    "valid_path = f'../../../data/{LANG.name}_{seg.id}_test.txt'\n",
    "\n",
    "# run the model \n",
    "!rm -rf $model_path \\\n",
    "  && mkdir $model_path \\\n",
    "  && cd $model_path \\\n",
    "  && ../../../rnnlm-0.3e/rnnlm \\\n",
    "    -train $train_path \\\n",
    "    -valid $valid_path \\\n",
    "    -rnnlm model \\\n",
    "    -hidden $hidden \\\n",
    "    -rand-seed $rand_seed \\\n",
    "    -debug $debug \\\n",
    "    -bptt $bptt \\\n",
    "    -class $_class"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: ../../rnnlm-0.3e/rnnlm: No such file or directory\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('snlp-vKRMqmdq': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "0ba93cb785b08a5c388f5a645f2c192b35e18c76a279f6b816e8452dcbc7cdb2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}