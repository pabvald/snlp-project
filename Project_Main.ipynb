{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab00522-feaf-4b57-97e6-9c56a2324832",
   "metadata": {},
   "source": [
    "# Import basic libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "881a46d8-9335-4c04-8ffe-de0fffb832ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import sentencepiece as spm\n",
    "\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import English, Bengali\n",
    "\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ac352-0a25-4b8b-be89-7a9f68e774ef",
   "metadata": {},
   "source": [
    "# Set a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "523163f8-8d49-49d4-9b32-d77a5d8891c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = English # English, Bengali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4f3d9-7efd-4a2d-b60c-a83a725be873",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e14aac0-11c8-40f5-8709-7f656df447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and dependencies\n",
    "import preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "from preprocessing import raw_preprocess, split_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa22e85-1a50-4526-a17d-c16ad370f39a",
   "metadata": {},
   "source": [
    "## 1.1. Preprocess data for sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "34ca06da-8631-484d-bee0-7f09cff306a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess raw data to get a list of sentences.\n",
    "sentences = raw_preprocess(LANG)\n",
    "\n",
    "# write the sentences into a file, each sentence on one line.\n",
    "with open(f'data/{LANG.name}_preprocessed.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(f'{sentence}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf9ef7-0ada-4279-9f68-1f402da7aaa6",
   "metadata": {},
   "source": [
    "## 1.2. Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fab171ab-957a-4c70-93d6-2d71256dd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test.\n",
    "train, test = split_train_test(sentences, LANG)\n",
    "\n",
    "# write train and test data into corresponding files.\n",
    "train_path = f'data/{LANG.name}_train.txt'\n",
    "test_path = f'data/{LANG.name}_test.txt'\n",
    "\n",
    "with open(train_path, 'w') as f:\n",
    "    f.write('\\n'.join(train))\n",
    "with open(test_path, 'w') as f:\n",
    "    f.write('\\n'.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393be75-2982-41be-b252-b5ac9bac68c5",
   "metadata": {},
   "source": [
    "# 2. Subword segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f77ebc7c-9356-47e6-a1a5-dbf1863102da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: experiment to get the best vocab_size for subword segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b8799da8-e821-4634-a7de-7fdff8e8b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and dependencies\n",
    "from collections import namedtuple\n",
    "import segmentation\n",
    "importlib.reload(segmentation)\n",
    "from segmentation import train_segmentation, encode_text_file, decode_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dbeb74ae-e39a-4cbb-b519-052eaa422732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a profile for each granularity level.\n",
    "Segmentation_config = namedtuple('Segmentation_config', 'id vocab_size, model_type')\n",
    "seg_profiles = [\n",
    "    Segmentation_config('s1', None, 'char'), # segmentation by characters.\n",
    "    Segmentation_config('s2', 500, 'bpe'), # segmentation by subwords with small vocabulary.\n",
    "    Segmentation_config('s3', 2000, 'bpe'), # segmentation by subwords with large vocabulary.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9f37700-97f4-4c96-9479-89c3bd02c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg in seg_profiles:\n",
    "    # do segmentation on the train data\n",
    "    spm1 = train_segmentation(\n",
    "        text_file=train_path, \n",
    "        LANG=LANG, \n",
    "        vocab_size=seg.vocab_size, \n",
    "        model_type=seg.model_type)\n",
    "\n",
    "    encoded_file = f'data/{LANG.name}_{seg.id}.txt'\n",
    "    encode_text_file(text_file=train_path, model_path=spm1, output_file=encoded_file)\n",
    "    decoded_file = f'data/{LANG.name}_{seg.id}_decoded.txt'\n",
    "    decode_text_file(text_file=encoded_file, model_path=spm1, output_file=decoded_file)\n",
    "    \n",
    "    # do segmentation on the test data\n",
    "    spm1 = train_segmentation(\n",
    "        text_file=test_path, \n",
    "        LANG=LANG, \n",
    "        vocab_size=seg.vocab_size, \n",
    "        model_type=seg.model_type)\n",
    "\n",
    "    encoded_file = f'data/{LANG.name}_{seg.id}_test.txt'\n",
    "    encode_text_file(text_file=train_path, model_path=spm1, output_file=encoded_file)\n",
    "    decoded_file = f'data/{LANG.name}_{seg.id}_decoded_test.txt'\n",
    "    decode_text_file(text_file=encoded_file, model_path=spm1, output_file=decoded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e25470-f690-460e-980d-cff2d3025bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: comment briefly on what you observe in terms of word segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2507b-5a61-4aaa-89af-4264517ba534",
   "metadata": {},
   "source": [
    "# 3. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "775b2dbc-b774-4e34-8e19-9d4b6c137522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug mode: 2\n",
      "train file: ../../data/en_s1.txt\n",
      "valid file: ../../data/en_s1_test.txt\n",
      "class size: 9999\n",
      "Hidden layer size: 40\n",
      "BPTT: 3\n",
      "Rand seed: 1\n",
      "rnnlm file: model\n",
      "Starting training using file ../../data/en_s1.txt\n",
      "Vocab size: 72\n",
      "Words in train file: 115565\n",
      "WARNING: number of classes exceeds vocabulary size!\n",
      "Iter:   0\tAlpha: 0.100000\t   TRAIN entropy: 3.2465    Words/sec: 1323.4   VALID entropy: 2.8076\n",
      "Iter:   1\tAlpha: 0.100000\t   TRAIN entropy: 2.7571    Words/sec: 1365.8   VALID entropy: 2.6472\n",
      "Iter:   2\tAlpha: 0.100000\t   TRAIN entropy: 2.6525    Words/sec: 1410.2   VALID entropy: 2.5773\n",
      "Iter:   3\tAlpha: 0.100000\t   TRAIN entropy: 2.5983    Words/sec: 1328.1   VALID entropy: 2.5351\n",
      "Iter:   4\tAlpha: 0.100000\t   TRAIN entropy: 2.5616    Words/sec: 1313.6   VALID entropy: 2.5055\n",
      "Iter:   5\tAlpha: 0.100000\t   TRAIN entropy: 2.5380    Words/sec: 1372.2   VALID entropy: 2.4962\n",
      "Iter:   6\tAlpha: 0.100000\t   TRAIN entropy: 2.5222    Words/sec: 1413.5   VALID entropy: 2.4884\n",
      "Iter:   7\tAlpha: 0.100000\t   TRAIN entropy: 2.5083    Words/sec: 1415.6   VALID entropy: 2.4798\n",
      "Iter:   8\tAlpha: 0.100000\t   TRAIN entropy: 2.5000    Words/sec: 1383.0   VALID entropy: 2.4663\n",
      "Iter:   9\tAlpha: 0.100000\t   TRAIN entropy: 2.4923    Words/sec: 1316.8   VALID entropy: 2.4612\n",
      "Iter:  10\tAlpha: 0.050000\t   TRAIN entropy: 2.3870    Words/sec: 1368.8   VALID entropy: 2.3514\n",
      "Iter:  11\tAlpha: 0.025000\t   TRAIN entropy: 2.3299    Words/sec: 1407.4   VALID entropy: 2.3062\n",
      "Iter:  12\tAlpha: 0.012500\t   TRAIN entropy: 2.2999    Words/sec: 1396.0   VALID entropy: 2.2844\n",
      "Iter:  13\tAlpha: 0.006250\t   TRAIN entropy: 2.2837    Words/sec: 1309.8   VALID entropy: 2.2736\n",
      "Iter:  14\tAlpha: 0.003125\t   TRAIN entropy: 2.2747    Words/sec: 1410.4   VALID entropy: 2.2686\n"
     ]
    }
   ],
   "source": [
    "!rm -rf models/rnnlm \\\n",
    "  && mkdir models/rnnlm \\\n",
    "  && cd models/rnnlm \\\n",
    "  && ../../rnnlm-0.3e/rnnlm \\\n",
    "    -train ../../data/en_s1.txt \\\n",
    "    -valid ../../data/en_s1_test.txt \\\n",
    "    -rnnlm model \\\n",
    "    -hidden 40 \\\n",
    "    -rand-seed 1 \\\n",
    "    -debug 2 \\\n",
    "    -bptt 3 \\\n",
    "    -class 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a267095-2d5d-4365-9f17-3367abcfe558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
